# @package models

# base config is `configs/models/multimodal_temporal/no_action.yaml`

image_encoder: i_jepa_target_encoder # Alias for ImageEncodingAgent.

i_jepa_target_encoder:
  _target_: ami.models.model_wrapper.ModelWrapper
  default_device: ${devices.0}
  has_inference: True
  inference_forward:
    _target_: ami.models.bool_mask_i_jepa.EncoderInferAveragePool
    n_patches:
      - >-
        ${python.eval:"
        ${shared.image_height} //
        ${models.i_jepa_target_encoder.model.patch_size}
        "}
      - >-
        ${python.eval:"
        ${shared.image_width} //
        ${models.i_jepa_target_encoder.model.patch_size}
        "}
    kernel_size: 1
    stride: null
  model:
    _target_: ami.models.bool_mask_i_jepa.BoolMaskIJEPAEncoder
    img_size:
      - ${shared.image_height} # Assuming 144
      - ${shared.image_width} # Assuming 144
    in_channels: ${shared.image_channels} # Assume 3
    patch_size: 12
    embed_dim: 216
    out_dim: 32 # 1 / 13.5
    depth: 6
    num_heads: 3
    mlp_ratio: 4.0

i_jepa_context_encoder:
  _target_: ami.models.model_wrapper.ModelWrapper
  default_device: ${devices.0}
  has_inference: False
  model: ${..i_jepa_target_encoder.model}

i_jepa_predictor:
  _target_: ami.models.model_wrapper.ModelWrapper
  default_device: ${devices.0}
  has_inference: False
  model:
    _target_: ami.models.bool_mask_i_jepa.BoolTargetIJEPAPredictor
    n_patches:
      - >-
        ${python.eval:"
        ${shared.image_height} //
        ${models.i_jepa_target_encoder.model.patch_size}
        "}
      - >-
        ${python.eval:"
        ${shared.image_width}
        // ${models.i_jepa_target_encoder.model.patch_size}
        "}
    context_encoder_out_dim: ${models.i_jepa_target_encoder.model.out_dim}
    hidden_dim: 108
    depth: 6
    num_heads: 2

multimodal_temporal_encoder:
  _target_: ami.models.model_wrapper.ModelWrapper
  default_device: ${devices.0}
  has_inference: True
  inference_forward:
    _target_: hydra.utils.get_method
    path: ami.models.temporal_encoder.inference_forward
  model:
    _target_: ami.models.temporal_encoder.MultimodalTemporalEncoder
    observation_flattens:
      image:
        _target_: ami.models.components.stacked_features.LerpStackedFeatures
        dim_in: ${models.i_jepa_target_encoder.model.out_dim}
        dim_out: 1024
        num_stack: >-
          ${python.eval:"
          (${models.i_jepa_predictor.model.n_patches.0}
          // ${models.i_jepa_target_encoder.inference_forward.kernel_size})
          * (${models.i_jepa_predictor.model.n_patches.1}
          // ${models.i_jepa_target_encoder.inference_forward.kernel_size})
          "}
    flattened_obses_projection:
      _target_: torch.nn.Linear
      in_features: >-
        ${python.eval:"
        ${..observation_flattens.image.dim_out}
        "}
      out_features: ${..core_model.dim}
    core_model:
      _target_: ami.models.components.sioconvps.SioConvPS
      depth: 6
      dim: 1024
      dim_ff_hidden: ${python.eval:"${.dim} * 2"}
      dropout: 0.1
    obs_hat_dist_heads:
      image:
        _target_: torch.nn.Sequential
        _args_:
          - _target_: ami.models.components.stacked_features.ToStackedFeatures
            dim_in: ${.....core_model.dim}
            dim_out: ${models.i_jepa_target_encoder.model.out_dim}
            num_stack: ${.....observation_flattens.image.num_stack}
          - _target_: ami.models.components.fully_connected_fixed_std_normal.FullyConnectedFixedStdNormal
            dim_in: ${..0.dim_out}
            dim_out: ${models.i_jepa_target_encoder.model.out_dim}
            normal_cls: Deterministic

forward_dynamics:
  _target_: ami.models.model_wrapper.ModelWrapper
  default_device: ${devices.0}
  has_inference: True
  model:
    _target_: ami.models.forward_dynamics.ForwardDynamcisWithActionReward
    observation_flatten:
      _target_: torch.nn.Identity
    action_flatten:
      _target_: ami.models.components.multi_embeddings.MultiEmbeddings
      choices_per_category:
        _target_: hydra.utils.get_object
        path: ami.interactions.environments.actuators.vrchat_osc_discrete_actuator.ACTION_CHOICES_PER_CATEGORY
      embedding_dim: 8
      do_flatten: True
    obs_action_projection:
      _target_: torch.nn.Linear
      # action_embedding_dim * num_action_choices + obs_embedding_dim
      in_features: >-
        ${python.eval:"
        ${models.multimodal_temporal_encoder.model.core_model.dim}
        + ${..action_flatten.embedding_dim} * 5
        "}
      out_features: ${..core_model.dim}
    core_model:
      _target_: ami.models.components.sioconvps.SioConvPS
      depth: 6
      dim: 1024
      dim_ff_hidden: ${python.eval:"${.dim} * 2"}
      dropout: 0.1
    obs_hat_dist_head:
      _target_: ami.models.components.fully_connected_fixed_std_normal.FullyConnectedFixedStdNormal
      dim_in: ${..core_model.dim}
      dim_out: ${models.multimodal_temporal_encoder.model.core_model.dim}
      normal_cls: Deterministic
    action_hat_dist_head:
      _target_: ami.models.components.discrete_policy_head.DiscretePolicyHead
      dim_in: ${..core_model.dim}
      action_choices_per_category:
        _target_: hydra.utils.get_object
        path: ami.interactions.environments.actuators.vrchat_osc_discrete_actuator.ACTION_CHOICES_PER_CATEGORY
    reward_hat_dist_head:
      _target_: ami.models.components.fully_connected_normal.FullyConnectedNormal
      dim_in: ${..core_model.dim}
      dim_out: 1
      squeeze_feature_dim: True

policy_value:
  _target_: ami.models.model_wrapper.ModelWrapper
  default_device: ${devices.0}
  has_inference: True
  model:
    _target_: ami.models.policy_value_common_net.PolicyValueCommonNet
    observation_projection:
      _target_: torch.nn.Linear
      in_features: ${models.multimodal_temporal_encoder.model.core_model.dim}
      out_features: ${..observation_hidden_projection.dim}
    forward_dynamics_hidden_projection:
      _target_: ami.models.components.stacked_hidden_state.StackwiseLinear
      in_stacks: ${models.forward_dynamics.model.core_model.depth}
      out_stacks: ${..observation_hidden_projection.depth}
    observation_hidden_projection:
      _target_: ami.models.components.sioconvps.SioConvPS
      depth: 6
      dim: 1024
      dim_ff_hidden: ${python.eval:"${.dim} * 2"}
      dropout: 0.1
    core_model:
      _target_: torch.nn.Identity
    policy_head:
      _target_: ami.models.components.discrete_policy_head.DiscretePolicyHead
      dim_in: ${..observation_hidden_projection.dim}
      action_choices_per_category:
        _target_: hydra.utils.get_object
        path: ami.interactions.environments.actuators.vrchat_osc_discrete_actuator.ACTION_CHOICES_PER_CATEGORY
    value_head:
      _target_: ami.models.components.fully_connected_value_head.FullyConnectedValueHead
      dim_in: ${..observation_hidden_projection.dim}
      squeeze_value_dim: True
