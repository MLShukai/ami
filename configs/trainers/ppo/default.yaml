_target_: ami.trainers.ppo_policy_trainer.PPOPolicyTrainer

partial_dataloader:
  _target_: torch.utils.data.DataLoader
  _partial_: true
  batch_size: 8
  shuffle: true

partial_optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4
  betas: [0.9, 0.999]

logger:
  _target_: ami.tensorboard_loggers.StepIntervalLogger
  log_dir: ${paths.tensorboard_dir}/ppo_policy
  log_every_n_steps: 1

device: ${devices.0}
max_epochs: 3
minimum_dataset_size: ${python.eval:"${data_collectors.ppo_trajectory.max_len} - 1"}
entropy_coef: 0.001
