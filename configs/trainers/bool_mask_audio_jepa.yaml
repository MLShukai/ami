defaults:
  - audio_jepa: bool_mask

audio_jepa_latent_auralization_context:
  _target_: ami.trainers.hifigan_trainer.HifiGANTrainer
  generator_name: hifigan_context_auralization_generator
  mel_spectrogram:
    _target_: torchaudio.transforms.MelSpectrogram
    sample_rate: ${shared.sample_rate}
    n_fft: 1024
    win_length: 1024
    hop_length: 256
    n_mels: 128
    window_fn: torch.hann_window
    center: False
    pad_mode: "reflect"
    normalized: False
  rec_coef: 45.0

  partial_dataloader:
    _target_: torch.utils.data.DataLoader
    _partial_: true
    batch_size: ${trainers.audio_jepa.partial_dataloader.batch_size}
    shuffle: true
    drop_last: true

  partial_optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.0001
    weight_decay: 0.01

  logger:
    _target_: ami.tensorboard_loggers.StepIntervalLogger
    log_dir: ${paths.tensorboard_dir}/audio_jepa_latent_auralization_context
    log_every_n_steps: 1

  # validation_dataloader:
  #   _target_: torch.utils.data.DataLoader
  #   batch_size: 128
  #   shuffle: false
  #   drop_last: false
  #   dataset:
  #     _target_: ami.trainers.components.vision.IntervalSamplingImageDataset
  #     image_dir: ${paths.data_dir}/random_observation_action_log/validation
  #     num_sample: 512
  #     pre_loading: true
  #     transform:
  #       _target_: torchvision.transforms.v2.Compose
  #       transforms:
  #         - _target_: torchvision.transforms.v2.Resize
  #           size:
  #             - ${shared.image_height}
  #             - ${shared.image_width}
  #         - _target_: ami.trainers.components.vision.Standardization

  device: ${devices.0}
  max_epochs: ${trainers.audio_jepa.max_epochs}
  minimum_dataset_size: ${.partial_dataloader.batch_size}
  minimum_new_data_count: ${trainers.audio_jepa.minimum_new_data_count}

audio_jepa_latent_auralization_target:
  _target_: ami.trainers.hifigan_trainer.HifiGANTrainer
  generator_name: hifigan_context_auralization_generator
  mel_spectrogram:
    _target_: torchaudio.transforms.MelSpectrogram
    sample_rate: ${shared.sample_rate}
    n_fft: 1024
    win_length: 1024
    hop_length: 256
    n_mels: 128
    window_fn: torch.hann_window
    center: False
    pad_mode: "reflect"
    normalized: False
  rec_coef: 45.0

  partial_dataloader:
    _target_: torch.utils.data.DataLoader
    _partial_: true
    batch_size: ${trainers.audio_jepa.partial_dataloader.batch_size}
    shuffle: true
    drop_last: true

  # validation_dataloader:
  #   _target_: torch.utils.data.DataLoader
  #   batch_size: 128
  #   shuffle: false
  #   drop_last: false
  #   dataset:
  #     _target_: ami.trainers.components.vision.IntervalSamplingImageDataset
  #     image_dir: ${paths.data_dir}/random_observation_action_log/validation
  #     num_sample: 512
  #     pre_loading: true
  #     transform:
  #       _target_: torchvision.transforms.v2.Compose
  #       transforms:
  #         - _target_: torchvision.transforms.v2.Resize
  #           size:
  #             - ${shared.image_height}
  #             - ${shared.image_width}
  #         - _target_: ami.trainers.components.vision.Standardization

  partial_optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.0001
    weight_decay: 0.01

  logger:
    _target_: ami.tensorboard_loggers.StepIntervalLogger
    log_dir: ${paths.tensorboard_dir}/audio_jepa_latent_auralization_target
    log_every_n_steps: 1

  device: ${devices.0}
  max_epochs: ${trainers.audio_jepa.max_epochs}
  minimum_dataset_size: ${.partial_dataloader.batch_size}
  minimum_new_data_count: ${trainers.audio_jepa.minimum_new_data_count}
