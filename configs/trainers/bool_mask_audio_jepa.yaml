defaults:
  - audio_jepa: default

# To reduce VRAM consumption, auralize only target encoder.
audio_jepa_latent_auralization_target:
  _target_: ami.trainers.hifigan_trainer.HifiGANTrainer
  generator_name: hifigan_target_auralization_generator
  mel_spectrogram:
    _target_: torchaudio.transforms.MelSpectrogram
    sample_rate: ${shared.audio_sample_rate}
    n_fft: 1024
    win_length: 1024
    hop_length: 256
    n_mels: 128
    window_fn:
      _target_: torch.hann_window
      _partial_: true
    center: False
    pad_mode: "reflect"
    normalized: False
  rec_coef: 45.0

  partial_dataloader:
    _target_: torch.utils.data.DataLoader
    _partial_: true
    batch_size: ${trainers.audio_jepa.partial_dataloader.batch_size}
    shuffle: true
    drop_last: true

  partial_optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.0002
    weight_decay: 0.01

  logger:
    _target_: ami.tensorboard_loggers.StepIntervalLogger
    log_dir: ${paths.tensorboard_dir}/audio_jepa_latent_auralization_target
    log_every_n_steps: 1

  validation_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 128
    shuffle: false
    drop_last: false
    dataset:
      _target_: ami.trainers.components.auditory.IntervalSamplingAudioDataset
      audio_dir: ${paths.data_dir}/random_observation_action_log.20241123/validation/audio/
      sample_rate: ${shared.audio_sample_rate}
      sample_size: ${shared.audio_sample_size}
      num_select: 512
      pre_loading: true

  device: ${devices.0}
  max_epochs: ${trainers.audio_jepa.max_epochs}
  minimum_dataset_size: ${.partial_dataloader.batch_size}
  minimum_new_data_count: ${trainers.audio_jepa.minimum_new_data_count}
